{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow: Understanding Tensors and Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow programming seems to be a very different beast altogether (I don't know Theano, but seems like [Theano](https://medium.com/@sentimentron/faceoff-theano-vs-tensorflow-e25648c31800#.z812n2u3r) is a similar meta-programming language). To understand Tensorflow we need to understand the two basic things that Google tried to incorporate as a Deep Learning (specifically till now) as a workflow -- __Tensors and Graphs__.\n",
    "\n",
    "***\n",
    "\n",
    "### Tensors:\n",
    "\n",
    "![Tensors](https://upload.wikimedia.org/wikipedia/commons/4/45/Components_stress_tensor.svg)\n",
    "\n",
    "Tensors as per the [Wiki](https://en.wikipedia.org/wiki/Tensor) definition are:\n",
    "> Tensors are geometric objects that describe linear relations between geometric vectors, scalars, and other tensors. Elementary examples of such relations include the dot product, the cross product, and linear maps. Geometric vectors, often used in physics and engineering applications, and scalars themselves are also tensors.\n",
    "\n",
    "From the list of imaginary constructs that Tensors can be moulded, Deep Learning needs us to think `Tensors` as **Multidimensional Arrays**.\n",
    "\n",
    "In a recent talk by my colleague, he had to show the difference between a Neural Network made in Numpy and Tensors. While creating the material for the talk, he observed that numpy and tensors take almost the same time to run (with different optimizers). We both had a lot of headache wrapping our heads around the same concept.\n",
    "\n",
    "To understand about tensors and numpy and why we needed tensors in the first place, when numpy was blazing fast (for python folks). The official NumPy landing page describes its usage:\n",
    "> NumPy can also be used as an efficient multi-dimensional container of generic data. Arbitrary data-types can be defined. This allows NumPy to seamlessly and speedily integrate with a wide variety of databases.\n",
    "\n",
    "So the question is, _What's the difference between Tensors and N-Dimensional Arrays that we can get from NumPy?_ \n",
    "\n",
    "- (I deeply regretted asking question myself after copy pasting the definition of Tensors). So in this [Math StackExchange post](http://math.stackexchange.com/questions/1134809/are-there-any-differences-between-tensors-and-multidimensional-arrays) this confusion has been cleared in a beautiful way.\n",
    "\n",
    "![Simply Tensors meme](https://cdn.meme.am/cache/instances/folder607/62245607.jpg)\n",
    "\n",
    ">Tensor : Multidimensional array :: Linear transformation : Matrix.\n",
    "\n",
    ">The short of it is, tensors and multidimensional arrays are different types of object; the first is a type of function, the second is a data structure suitable for representing a tensor in a coordinate system.\n",
    "\n",
    ">In the sense you're asking, mathematicians usually define a \"tensor\" to be a multilinear function: a function of several vector variables that is \"linear in each variable separately\". A \"tensor field\" is a \"tensor-valued function\".\n",
    "    \n",
    ">For a more mathematically rigourous explaination, please visit [this link](http://math.stackexchange.com/questions/10282/an-introduction-to-tensors?noredirect=1&lq=1)\n",
    "    \n",
    "So basically tensors are functions or containers that we need to define. The actual calculation happens when there's data fed (lazy calculations). What we see as numpy arrays (1D, 2D, ..., ND) can be considered as generic tensors ([Refer Slide 7](https://cs224d.stanford.edu/lectures/CS224d-Lecture7.pdf))\n",
    "    \n",
    "While writing this post, by this time I do have a clarity on what are Tensors. The next question(s) that crops up is _why is it a big deal in Tensorflow. Do tensors flow? How do they flow? rather, How do I make them flow? Also, NumPy isn't that bad, no?_\n",
    "\n",
    "**Disclaimer: Writing a post on Tensorflow well past midnight isn't a rather good idea**\n",
    "\n",
    "- So what NumPy array lacks is creating Tensors ([Refer Slide 8](https://cs224d.stanford.edu/lectures/CS224d-Lecture7.pdf)). Hmmm, there's a way though to convert tensors to numpy and vice-versa (That should be possible since the constructs are defined definitely as arrays/matrices).\n",
    "\n",
    "    [This is a good resource](https://hackernoon.com/learning-ai-if-you-suck-at-math-p4-tensors-illustrated-with-cats-27f0002c9b32#.92vyu9gyl) if you want to have a layman's understanding of Tensors and in a relatable way to NumPy arrays.\n",
    "    \n",
    "I could get a few answers reading and searching for tensors and numpy arrays. I hope things are clear at this point, after a rather brief, non-haphazarous way(hopefully). \n",
    "\n",
    "***\n",
    "\n",
    "### Graphs:\n",
    "\n",
    "Theano's meta-programming structure seems to be an inspiration for Google to create Tensorflow, but folks at Google took it to a next level with their media darling.\n",
    "\n",
    "According to the official Tensorflow blog on _Getting Started_:\n",
    "> The Computational Graph\n",
    "You might think of TensorFlow Core programs as consisting of two discrete sections:\n",
    "\n",
    "> - Building the computational graph.\n",
    "\n",
    "> - Running the computational graph.\n",
    "\n",
    "A computational graph is a series of TensorFlow operations arranged into a graph of nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The multiplication produces::: 6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# If we consider a simple multiplication\n",
    "a = 2\n",
    "b = 3\n",
    "mul = a*b \n",
    "\n",
    "print (\"The multiplication produces:::\", mul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The multiplication produces::: Tensor(\"Mul:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# But consider a tensorflow program to replicate above\n",
    "at = tf.constant(3)\n",
    "bt = tf.constant(4)\n",
    "\n",
    "mult = tf.mul(at, bt)\n",
    "\n",
    "print (\"The multiplication produces:::\", mult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Each node takes zero or more tensors as inputs and produces a tensor as an output. One type of node is a constant. Like all TensorFlow constants, it takes no inputs, and it outputs a value it stores internally. \n",
    "\n",
    "I think the above statement holds true as we have seen that constructing a computational graph to multiply two values is rather a straight forward task. But we need the value at the end. We have defined the two constants, `at` and `bt`, along with their values. What if we don't define the values? Let's check\n",
    "\n",
    "** To read about Graphs: [Click here](https://relinklabs.com/the-power-of-graph-analytics-1-1)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "constant() missing 1 required positional argument: 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3d0aff390325>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: constant() missing 1 required positional argument: 'value'"
     ]
    }
   ],
   "source": [
    "at = tf.constant()\n",
    "bt = tf.constant()\n",
    "\n",
    "mult = tf.mul(at, bt)\n",
    "\n",
    "print (\"The multiplication produces:::\", mult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I guess the constant needs a value (duh!). Next step would be to find out why we didn't get the output. It seems that to evaluate the graph that we made, it needs to be run in a `session`. \n",
    "\n",
    "To understand this complexity, we need to understand what _our_ computational graph has:\n",
    "\n",
    "- Tensors: at, bt\n",
    "- Operations: mult\n",
    "\n",
    "To execute `mult`, the computational graph needs a `session` where the tensors and operations would be evaluated. Let's now evaluate our graph in a session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The actual multiplication result::: 12\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "# Executing the session\n",
    "print (\"The actual multiplication result:::\", sess.run(mult))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above graph would print the same value since we are using constants. There are 2 more ways we could send values to the graph - _Variables_ and _Placeholders_\n",
    "\n",
    "##### Variables:\n",
    "\n",
    "> When you train a model, you use variables to hold and update parameters. Variables are in-memory buffers containing tensors. They must be explicitly initialized and can be saved to disk during and after training. You can later restore saved values to exercise or analyze the model. \n",
    "\n",
    ">Variable initializers must be run explicitly before other ops in your model can be run. The easiest way to do that is to add an op that runs all the variable initializers, and run that op before using the model.\n",
    "\n",
    "> - Source: Tensorflow Docs\n",
    "\n",
    "We can initialize variables from another variables too, that's a plus! Constants can't be updated, that's a shame everywhere. Need to check whether dynamically variables can be created. \n",
    "\n",
    "##### Placeholders:\n",
    "\n",
    ">TensorFlow provides a placeholder operation that must be fed with data on execution. \n",
    "\n",
    ">tf.placeholder(dtype, shape=None, name=None)\n",
    "\n",
    ">Inserts a placeholder for a tensor that will be always fed.\n",
    "\n",
    "So from the reading in [this post](http://learningtensorflow.com/lesson4/), we can conclude that placeholders is a way to define variables without actually defining the values to be passed to it when we create a computational graph.\n",
    "\n",
    "`tf.placeholder()` is the norm, used by all the Tensorflow folks writing code daily. \n",
    "\n",
    "For a more indepth reading: [I/O for Tensorflow](https://www.tensorflow.org/api_guides/python/io_ops)\n",
    "\n",
    "The below image provides an overview of how Tensorflow works on the data [Source: Medium.com|[Article](https://hackernoon.com/machine-learning-with-tensorflow-8873fdee2b68#.foefoqaxs)]\n",
    "![Tensorflow](https://cdn-images-1.medium.com/max/800/1*2mI_CfSOhyl0if-o-d7O4A.png)\n",
    "\n",
    "<html><p> <br/></p></html>\n",
    "<html><p> <br/></p></html>\n",
    "We would check out `Variables` and `Placeholders` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Mul_2:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Variable\n",
    "\n",
    "var1 = tf.Variable(2, name=\"var1\")\n",
    "var2 = tf.Variable(3, name=\"var2\")\n",
    "\n",
    "mulv = tf.mul(var1, var2)\n",
    "\n",
    "print (mulv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The variable var1 is::: 2\n",
      "The variable var2 is::: 2\n",
      "The computational result is::: 6\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer()) # always need to initialize the variables\n",
    "    \n",
    "    print (\"The variable var1 is:::\", sess.run(var1))\n",
    "    print (\"The variable var2 is:::\", sess.run(var1))\n",
    "    print (\"The computational result is:::\", sess.run(mulv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Add_1:0\", dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Placeholder\n",
    "\n",
    "pl = tf.placeholder(tf.float32, name=\"p\")\n",
    "pi = tf.constant(3.)\n",
    "c = tf.add(pl, pi)\n",
    "\n",
    "print (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-15-4c5578691c20>:3 in <module>.: SummaryWriter.__init__ (from tensorflow.python.training.summary_io) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.FileWriter. The interface and behavior is the same; this is just a rename.\n",
      "The calculation result is::: 6.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer()) # always need to initialize the variables\n",
    "    writer = tf.train.SummaryWriter(\"output\", sess.graph)\n",
    "    #print(\"The placeholder value passed:::\", sess.run(pl, {pl:3}))\n",
    "    print(\"The calculation result is:::\", sess.run(c, {pl:3}))\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Tensorboard we get the follow graph:\n",
    "\n",
    "![Graph](https://raw.githubusercontent.com/pratos/pratos.github.io/master/images/graph-run%3D.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far what we have seen in this post is basic Tensors and what do these do in a computational graph. The actual objective of creating this is to make _Tensors flow through the graph_. We write the tensors and through sessions we make them flow. \n",
    "\n",
    "Tensorflow Wiki describes it as: **A Stateful Dataflow Language**\n",
    "\n",
    "[This Y-Combinator forum](https://news.ycombinator.com/item?id=11597995) describes Tensorflow, which makes sense:\n",
    "\n",
    ">\t\n",
    "chimtim 301 days ago [-]\n",
    "\n",
    ">It is a dataflow language and deals with tensors (multi dim arrays), so tensor + dataflow = tensorflow.\n",
    "\n",
    "For now this post and research satiates me, when need for (or urge) arrises to answer my own questions would write one.\n",
    "\n",
    "**Readers do let me know whether this was helpful and do correct if anything is doubtful or completely wrong!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sources:\n",
    "1. [Tensorflow - Getting Started](https://www.tensorflow.org/get_started/get_started)\n",
    "2. [CS224d Slides](https://cs224d.stanford.edu/lectures/CS224d-Lecture7.pdf)\n",
    "3. [MetaFlow Blog](https://blog.metaflow.fr/tensorflow-a-primer-4b3fa0978be3#.jv0afaazw)\n",
    "4. [Theano vs Tensorflow](https://medium.com/@sentimentron/faceoff-theano-vs-tensorflow-e25648c31800#.shswqzv5r)\n",
    "5. [Machine Learning with Tensorflow](https://hackernoon.com/machine-learning-with-tensorflow-8873fdee2b68#.foefoqaxs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
